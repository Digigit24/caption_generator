# Video Caption Generator

High-precision video caption generation system using **Faster-Whisper** with the **large-v3** model for 90%+ accuracy.

## ğŸ—ï¸ Architecture

- **Backend**: Node.js (Express) for video uploads and processing
- **Storage**: Local filesystem with organized folders
- **Database**: SQLite for tracking chunk processing
- **Audio Processing**: FFmpeg for extraction and intelligent splitting
- **AI Engine**: Faster-Whisper (Python) with large-v3 model
- **Context Preservation**: Sequential processing with initial_prompt for anti-hallucination

## âœ¨ Features

- âœ… Intelligent audio splitting at silence points (no cut words)
- âœ… Sequential chunk transcription with context preservation
- âœ… 90%+ accuracy using Faster-Whisper large-v3
- âœ… Concurrent processing of multiple videos (configurable)
- âœ… Automatic SRT generation with precise timestamps
- âœ… REST API for easy integration
- âœ… Progress tracking and status monitoring
- âœ… Automatic cleanup of temporary files

## ğŸ› ï¸ Prerequisites

### Required Software

1. **Node.js** (v16 or later)
   ```bash
   node --version
   ```

2. **FFmpeg** (with full build)
   - **Windows**: Download from [ffmpeg.org](https://ffmpeg.org/download.html)
   - **Linux**: `sudo apt-get install ffmpeg`
   - **macOS**: `brew install ffmpeg`

3. **Python 3.10+**
   ```bash
   python3 --version
   ```

4. **Faster-Whisper Library**
   ```bash
   pip install faster-whisper
   ```

5. **CUDA Toolkit** (Optional, for GPU acceleration)
   - Only if you have NVIDIA GPU
   - Download from [NVIDIA](https://developer.nvidia.com/cuda-downloads)

## ğŸ“¦ Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd caption_generator
   ```

2. **Install Node.js dependencies**
   ```bash
   npm install
   ```

3. **Configure environment**
   Edit `.env` file:
   ```env
   PORT=3000
   CHUNK_DURATION=60
   MAX_CONCURRENT_VIDEOS=3
   WHISPER_MODEL=large-v3
   WHISPER_DEVICE=cpu          # Change to 'cuda' for GPU
   WHISPER_COMPUTE_TYPE=int8   # Use 'float16' for GPU
   ```

4. **Make Python script executable** (Linux/macOS)
   ```bash
   chmod +x transcribe.py
   ```

## ğŸš€ Usage

### Start the Server

```bash
npm start
```

Or with auto-reload during development:
```bash
npm run dev
```

The server will start on `http://localhost:3000`

### API Endpoints

#### 1. Upload Video

```bash
POST /api/upload
Content-Type: multipart/form-data

# Using curl:
curl -X POST http://localhost:3000/api/upload \
  -F "video=@/path/to/your/video.mp4"

# Response:
{
  "success": true,
  "videoId": "uuid-here",
  "filename": "video.mp4",
  "message": "Video uploaded successfully and queued for processing"
}
```

#### 2. Check Processing Status

```bash
GET /api/status/:videoId

# Using curl:
curl http://localhost:3000/api/status/uuid-here

# Response:
{
  "success": true,
  "video": {
    "id": "uuid-here",
    "filename": "video.mp4",
    "status": "transcribing",
    "createdAt": "2024-01-13 10:30:00"
  },
  "progress": {
    "totalChunks": 25,
    "completedChunks": 10,
    "percentage": 40
  },
  "captions": {
    "count": 145
  }
}
```

**Status Values:**
- `uploaded` - Video uploaded, waiting in queue
- `extracting_audio` - Extracting audio from video
- `splitting` - Splitting audio into chunks
- `transcribing` - Transcribing chunks
- `merging` - Merging captions into final SRT
- `completed` - Processing complete
- `failed` - Processing failed

#### 3. Check Queue Status

```bash
GET /api/queue

# Using curl:
curl http://localhost:3000/api/queue

# Response:
{
  "success": true,
  "queue": {
    "queueSize": 2,
    "activeProcesses": 3,
    "maxConcurrent": 3,
    "queuedVideos": ["uuid-1", "uuid-2"]
  }
}
```

#### 4. Download SRT File

```bash
GET /api/download/:videoId

# Using curl:
curl -O http://localhost:3000/api/download/uuid-here

# Downloads: video.mp4.srt
```

## ğŸ”§ How It Works

### Processing Workflow

```
1. Upload Video
   â†“
2. Extract Audio (WAV format, 16kHz mono)
   â†“
3. Detect Silence Points
   â†“
4. Split Audio at Silence (intelligent chunks ~60s)
   â†“
5. Transcribe Chunks Sequentially
   â”‚
   â”œâ”€ Chunk 1: Transcribe with no context
   â”œâ”€ Chunk 2: Transcribe with Chunk 1's last 100 chars
   â”œâ”€ Chunk 3: Transcribe with Chunk 2's last 100 chars
   â””â”€ ...
   â†“
6. Merge Captions with Timestamp Offsetting
   â”‚
   â”œâ”€ Chunk 0: timestamps as-is
   â”œâ”€ Chunk 1: add 60s to all timestamps
   â”œâ”€ Chunk 2: add 120s to all timestamps
   â””â”€ ...
   â†“
7. Generate Final SRT File
   â†“
8. Cleanup Temporary Files
```

### Key Precision Features

1. **Context Preservation**: Each chunk receives the last 100 characters from the previous chunk as `initial_prompt`, preventing hallucination at chunk boundaries.

2. **Intelligent Splitting**: Audio is split at silence points (not hard 60s cuts), preventing words from being cut in half.

3. **Large-v3 Model**: Uses the most accurate Whisper model for 90%+ precision.

4. **Sequential Processing**: Chunks are processed one-by-one to maintain perfect context flow.

## ğŸ“Š Performance

- **Accuracy**: 90%+ (using large-v3 model)
- **Speed (CPU)**: ~1-2x realtime (25min video = 25-50min processing)
- **Speed (GPU)**: ~10x realtime (25min video = 2-5min processing)
- **Concurrent Videos**: 2-3 recommended for home PC

## ğŸ“ Project Structure

```
caption_generator/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.js              # Express server
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ upload.js          # Upload endpoint
â”‚   â”‚   â””â”€â”€ status.js          # Status & download endpoints
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ processor.js       # Main processing queue
â”‚   â”‚   â””â”€â”€ transcription.js   # Python bridge
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ database.js        # SQLite operations
â”‚       â””â”€â”€ ffmpeg.js          # Audio processing
â”œâ”€â”€ transcribe.py              # Faster-Whisper script
â”œâ”€â”€ uploads/                   # Uploaded videos (temp)
â”œâ”€â”€ chunks/                    # Audio chunks (temp)
â”œâ”€â”€ captions/                  # Final SRT files
â”œâ”€â”€ captions.db               # SQLite database
â”œâ”€â”€ package.json
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

## ğŸ› Troubleshooting

### Common Issues

**1. "FFmpeg not found"**
- Ensure FFmpeg is installed and in your PATH
- Test: `ffmpeg -version`

**2. "Python script failed"**
- Ensure faster-whisper is installed: `pip install faster-whisper`
- Check Python path: `which python3`
- First run will download the large-v3 model (~3GB)

**3. "Out of memory"**
- Reduce `MAX_CONCURRENT_VIDEOS` in `.env`
- Use smaller model: `WHISPER_MODEL=medium`
- Use int8 quantization: `WHISPER_COMPUTE_TYPE=int8`

**4. "Model download failed"**
- The large-v3 model (~3GB) downloads on first use
- Ensure stable internet connection
- Models are cached in `~/.cache/huggingface/`

**5. "Slow transcription"**
- Install CUDA if you have NVIDIA GPU
- Set `WHISPER_DEVICE=cuda` and `WHISPER_COMPUTE_TYPE=float16`
- Or use a smaller model: `WHISPER_MODEL=medium`

## ğŸ” Security Notes

- This is designed for local/internal use
- Add authentication for production deployment
- Consider file size limits for your use case
- Sanitize uploaded filenames in production

## ğŸ“ License

MIT

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or PR.

## ğŸ“š References

- [Faster-Whisper](https://github.com/guillaumekln/faster-whisper)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [FFmpeg Documentation](https://ffmpeg.org/documentation.html)
